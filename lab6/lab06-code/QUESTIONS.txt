                           __________________

                            LAB 6 QUESTIONS
                           __________________

Lab Instructions
================

  Follow the instructions below to experiment with topics related to
  this lab.
  - For sections marked QUIZ, fill in an (X) for the appropriate
    response in this file. Use the command 'make test-quiz' to see if
    all of your answers are correct.
  - For sections marked CODE, complete the code indicated. Use the
    command 'make test-code' to check if your code is complete.
  - DO NOT CHANGE any parts of this file except the QUIZ sections as it
    may interfere with the tests otherwise.
  - If your 'QUESTIONS.txt' file seems corrupted, restore it by copying
    over the 'QUESTIONS.txt.bak' backup file.
  - When you complete the exercises, check your answers with 'make test'
    and if all is well, create a zip file with 'make zip' and upload it
    to Gradescope. Ensure that the Autograder there reflects your local
    results.
  - IF YOU WORK IN A GROUP only one member needs to submit and then add
    the names of their group.


QUIZ: print_department.c
========================

  First, review the print_department.c file, as many of the techniques it
  demonstrates will need to be used to complete this lab.

  Near the top of the program 'print_department.c', code like the
  following appears.
  ,----
  | file_header_t *header = (file_header_t *) file_bytes;
  `----

  Which of the below best describes what this line is doing?
  - (X) It sets a pointer to the same location as file_bytes but
    indicates that the data there should be treated as a file_header_t.
  - ( ) It does pointer arithmetic between file_bytes (pointer to
    beginning of the file) and offset to find the location where
    name/emails for an individual department are located in the file.
  - ( ) It changes the memory mapped file to be at a different location
    in memory and refer to a different type; subsequently it will be
    treated as file_header_t data.
  - ( ) It alters the header of the binary file by assigning new data to
    the mapped location in the file.


  One part of the program print_department.c uses a line that looks like
  the following
  ,----
  | contact_t *dept_contacts = (contact_t *) (file_bytes + offset);
  `----

  Which of the below best describes what this line is doing?
  - (X) It does pointer arithmetic between file_bytes (pointer to
    beginning of the file) and offset to find the location where
    name/emails for an individual department are located in the file.
  - ( ) It does pointer arithmetic to convert character data (a string)
    to be reformatted as a contact_t; this converts binary data to
    printable data.
  - ( ) It calculates how much of the file remains by summing the number
    of bytes in the file and the current offset; this is then
    dereferenced to get a struct pointer.
  - ( ) It changes the memory mapped file to be at a different location
    in memory and refer to a different type; subsequently it will be
    treated as contact_t data.


CODE: Complete 'email_lookup.c'
===============================

  Complete the code in the 'email_lookup.c' file which is intended to accept an
  email address on the command line and look up the Department / Name of the
  person to which it belongs.  Much of the code is provided but some lines are
  marked with 'TODO' comments and need you to add code to complete the
  implementation as instructed. Many of the required techniques have analogues
  in 'print_department.c', so adapt the code provided there as appropriate.

  When complete, the 'email_lookup' program will run as follows.
  ,----
  | >> ./email_lookup cse_depts.dat follstad@umn.edu
  | Found follstad@umn.edu: IT Department --> Carl Follstad
  |
  | >> ./email_lookup cse_depts.dat sjguy@umn.edu
  | Found sjguy@umn.edu: CS Department --> Stephen Guy
  |
  | >> ./email_lookup cse_depts.dat heimdahl@umn.edu
  | Found heimdahl@umn.edu: CS Department --> Mats Heimdahl
  |
  | >> ./email_lookup cse_depts.dat linus@umn.edu
  | Email 'linus@umn.edu' not found
  `----

INFO: Vectorized Operations
============================

  Modern processors feature special vector instructions that are executed to
  carray out work in parallel. Specifically, these instructions perform the
  same operation (arithmetic, logical, bitwise, etc.) on multiple values stored
  together in a "vector", making them an example of the "Single Instruction,
  Multiple Data" (SIMD) paradigm. The reason these instructions are valuable
  is because they can often complete in just one CPU cycle (ignoring
  pipelining), just like regular instructions.  In other words, they let the
  CPU do more work in the same amount of time as a regular instruction.

  State of the art implementations of numerical algorithms (like matrix
  multiplication) almost always use these instructions as they enable faster
  execution than one would be able to achieve with traditional techniques, like
  loop unrolling, alone.

  Intel has defined a C API for using these instructions featuring compiler
  "intrinsics" -- data types and functions that are very thin wrappers for
  assembly operations. In this lab, you will explore how these intrinsics are
  used and how they correspond to assembly code.

QUIZ: array_add.c
=================

  Open the 'array_add.c' file. It contains two functions that compute the
  element-wise sum of two arrays, one with a traditional for loop and addition
  and the other with SIMD intrinsics.


  Based on its usage in 'array_add_vec', what do you think the data type
  '__m256i' represents?

   - ( ) A vector of 256 integer values
   - (X) A vector of integers with size 256 bits
   - ( ) A matrix of 256 integer values
   - ( ) A matrix of integers with size 256 bits

   Which of the following intrinsics is used to copy integers from memory into a
   vector instance?

   - (X) _mm256_loadu_si256
   - ( ) _mm256_storeu_si256
   - ( ) _mm256_add_epi32
   - ( ) _mm256_mullo_epi32


   Which of the following intrinsics is used to copy integers from a vector
   instance to a memory address?

   - ( ) _mm256_loadu_si256
   - (X) _mm256_storeu_si256
   - ( ) _mm256_add_epi32
   - ( ) _mm256_mullo_epi32


   Which of the following intrinsics is used to compute the element-wise sum
   of two vectors containing 32-bit integer values?

   - ( ) _mm256_add_epi16
   - ( ) _mm256_mullo_epi16
   - (X) _mm256_add_epi32
   - ( ) _mm256_mullo_epi32


  What does the "vectorized" version of adding two arrays have in common with a
  loop-unrolled version?

  - ( ) Both will perform better if we use multiple accumulators.
  - (X) Both can only work on fixed-size chunks of data, so we need a cleanup
        loop at the end.
  - ( ) Both will have diminishing returns as chunk size increases beyond a
        certain point.
  - ( ) Trick Question: These two optimizations have nothing in commmon.


  How does the speed of the vectorized version of array addition compare to the
  traditional version?

  - ( ) The vectorized version is slower than the traditional version.
  - ( ) The vectorized version is about the same speed as the traditional
        version.
  - (X) The vectorized version is faster than the traditional version.
  - ( ) The relative speed of the two implementations is not consistent between
        different executions.

  Take a moment to study the assembly generated from 'array_add.c'. You can
  generate such assembly with the following command:

  > gcc -mavx2 -S array_add.c

  then look at the contents of 'array_add.s', particularly the instructions
  under the 'array_add_vec' label. What do you notice?

  - ( ) There are special vector-specific instructions starting with 'v', like
        'vmovdqu' and 'vpadd'.
  - ( ) There are special vector-specific CPU registers like '%xmm' and '%ymm'.
  - ( ) Some of these vector instructions take 3 or even 4 operands, unlike the
        typical assembly instructions we have studied previously.
  - (X) All of the above.


CODE: Complete 'array_dot.c'
=============================

  Complete the code in the 'array_dot.c' file. This is similar to the code in
  the 'array_add.c' file, except that we now have functions for traditional and
  vectorized versions of a dot product rather than an element-wise sum.

  Specifically, complete the implementation of the function 'array_dot_vec'.
  We've already implemented code to intialize arays for testing and time the
  speed of the two functions for you.

  A few important hints:

    1. Use the intrinsic '_mm256_mullo_epi32' to perform element-wise
       multiplication of vectors containing 32-bit integers.
    2. You'll want to make a temporary array to hold the results of these
       element-wise products, as is done in the provided 'array_dot' function.
    3. It's OK to compute your final sum, after all multiplication has been
       performed, with a plain old 'for' loop, like in the 'array_dot' function.

A Few Last Notes
================

  Congratulations! You've just completed the last lab assignment for CSCI 2021.
  A few notes on this lab's code and where to go from here if you want to
  explore vectorization in more detail.

  You probably noticed that the vectorized code's speedup was not as significant
  as one would expect. Part of this has to do with memory alignment. The vector
  load and store operations work best if they are working with memory addresses
  properly aligned to a vector's size. There are versions of load and store that
  run much faster if they can assume this alignment (e.g., _mm256_load_si256
  rather than _mm256_loadu_si256).

  Unfortunately, it takes some work to get malloc() to return 256-bit aligned
  addresses, although you could implement an aligned allocator yourself with
  mmap() and some bookkeeping code.

  The latest and greatest vector instructions use 512-bit vectors and are
  defined in the AVX-512 standard. These instructions are not yet supported on
  all modern CPUs (ironically, AMD's latest CPUs support them while Intel's do
  not) and have been the subject of some controversy.
